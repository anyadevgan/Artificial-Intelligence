{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkhCnnA-32fP"
      },
      "source": [
        "# Problem 1: Robot Localization (32 points)\n",
        "\n",
        "A robot is wandering around a room with some obstacles, labeled as $\\#$ in the grid below. It can occupy any of the free cells labeled with a letter, but we are uncertain about its true location and thus keep a belief distribution over its current location. At each timestep it moves from its current cell to a neighboring free cell in one of the four cardinal directions with uniform probability; it cannot stay in the same cell. For example, from A the robot can move to either B or C with probability $\\frac12$, while from D it can move to B, C, E, or F, each with probability $\\frac14$.\n",
        "\n",
        "![Picture1.png](data:image/png;base64,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)\n",
        "\n",
        "The robot also makes an observation after each transition, returning what it sees in a randomly chosen cardinal direction. Possibilities include observing $\\#$, \"wall\", or \"empty\" (for a free cell). For example, in C the robot observes \"wall\", $\\#$ (each with probability $\\frac14$), or \"empty\" (with probability $\\frac12$).\n",
        "\n",
        "**Note**: You don't have to show work for solving linear equations or eigenvectors, but please show what equations or matrices you use. Feel free to show your work in Python as well. You may also omit computations that will turn out to be zero based on the provided information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBwPiioasIfZ"
      },
      "source": [
        "1.  Suppose that the robot wanders around forever without making observations. What is the stationary distribution over the robot's predicted location? \n",
        "\n",
        "2. The initial distribution $X_0$ is uniform over all possible states. The robot makes a transition and observes $e_1 = $ \"wall\". It then makes a second transition and again observes $e_2 = $ \"wall\". What are the belief distributions $\\Pr(X_1 \\mid e_1)$ and $\\Pr(X_2 \\mid e_1, e_2)$?\n",
        "\n",
        "4. Compute the joint distribution $\\Pr(X_1, X_2 \\mid e_1, e_2)$. Hint: First determine the state sequences with nonzero probabilities.\n",
        "\n",
        "5. Are the states $X_1$ and $X_2$ independent given $e_1$ and $e_2$? Why or why not? What is the most likely state sequence(s) of $X_1$ and $X_2$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_c8qZSyuGcJ"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\n",
        "\n",
        "1.  \n",
        "T = \n",
        "\\begin{bmatrix}\n",
        "0 & 0.5 & 0.5 & 0 & 0 & 0\\\\\n",
        "0.5 & 0 & 0 & 0.25 & 0 & 0\\\\\n",
        "0.5 & 0 & 0 & 0.25 & 0 & 0\\\\\n",
        "0 & 0.5 & 0.5 & 0 & 1 & 1\\\\\n",
        "0 & 0 & 0 & 0.25 & 0 & 0\\\\\n",
        "0 & 0 & 0 & 0.25 & 0 & 0\\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "$P_{\\infty}(A) = 0.5P_{\\infty}(B) + 0.5P_{\\infty}(C)$ \\\\\n",
        "$P_{\\infty}(B) = 0.5P_{\\infty}(A) + 0.5P_{\\infty}(D)$ \\\\\n",
        "$P_{\\infty}(C) = 0.5P_{\\infty} + 0.25P_{\\infty}(D)$ \\\\\n",
        "$P_\\infty(D) = 0.5P_\\infty(B) + 0.5P_\\infty(C) + P_\\infty(E) + P_\\infty(F)$ \\\\\n",
        "$P_\\infty(E) = 0.25P_\\infty(D)$ \\\\\n",
        "$P_\\infty(F) = 0.25P_\\infty(D)$ \\\\\n",
        "\n",
        "$P_{\\infty}(A) = 1/6$ \\\\\n",
        "$P_{\\infty}(B) = 1/6$ \\\\\n",
        "$P_{\\infty}(C) = 1/6$ \\\\\n",
        "$P_{\\infty}(D) = 1/3$ \\\\\n",
        "$P_{\\infty}(E) = 1/12$ \\\\\n",
        "$P_{\\infty}(F) = 1/12$ \\\\\n",
        "\\\\\n",
        "\n",
        "2. \n",
        "$P(X_0) = [0.167, 0.167, 0.167, 0.167, 0.167, 0.167]$ \\\\\n",
        "$ùëÉ(ùëã_1) = ùëá * ùëÉ(ùëã_0) = [0.167, 0.125, 0.125, 0.4125, 0.04125, 0.04125]$ \\\\\n",
        "$ùëÉ(ùëã_1 | e_1) = [0.083, 0.031, 0.031, 0, 0.01, 0.01]$ \\\\\n",
        "Normalized: $[0.50, 0.188, 0.188, 0, 0.062, 0.062]$ \\\\\n",
        "$ùëÉ(ùëã_2 | e_1, e_2) = [0.5, 0.25, 0.25, 0, 0.25, 0.25] * [0.188, 0.25, 0.25, 0.312, 0, 0] = [0.429, 0.285, 0.285, 0, 0, 0]$ \\\\\n",
        "\n",
        "\n",
        "3. We can use the chain rule laws to rewrite $P(X_{1},X_{2}|e_{1},e_{2})$ as\n",
        "$P(e_{1}|X_{2})P(X_{1}|e_{1})P(e_{1})P(X_{2}|X_{1})$. \\\\\n",
        "$P(e_{1}|X_{2} ) = (.5, .25, .25, 0, .25, .25)$ \\\\\n",
        "$e_{1} = (0.083, 0.031, 0.031, 0, 0.010, 0.010)$\n",
        "\n",
        "\n",
        "4. The states $X_1$ and $X_2$ are not independent given $e_1$ and $e_2$ since step 2 depends on step 1. Consider the following example: if you are at state D, you can't get to state A because you are only able to move left, right, up, or down (not diagonally). Additionally, the states do not fulfill the following equations which are only true when $X_1$ and $X_2$ are independent: $P(X_1, X_2 | e_1, e_2) = P(X_1| e_1, e_2) = P(X_2 | e_1, e_2)$ The most likely sequences of $X_1$ and $X_2$ are  A, B, A, C."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgTXtGN4Ojtm"
      },
      "source": [
        "# ENTER ANY CODE FOR THE ABOVE PROBLEM HERE\n",
        "import numpy as np\n",
        "import numpy.linalg as nla\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DjvOWgavBGB"
      },
      "source": [
        "# Problem 2: Language Modeling (18 points)\n",
        "\n",
        "One task in probabilistic [language modeling](https://en.wikipedia.org/wiki/Language_model) is to compute the probability of a  word given a sequence of words. Each word in a sequence can be thought of as a random variable $X_i$. In a phrase or sentence of $m$ words, we thus have a sequence of random variables $X_1, ..., X_m$. The domain size of each of the random variables would be the size of our vocabulary $|X|$.\n",
        "\n",
        "1.  Suppose we want to predict a particular word $X_j$ given all other words in the sequence $X_1, ..., X_{j-1}, X_{j+1}, ..., X_m$. We use the **naive Bayes** approach. Write an analytical expression for the joint probability $\\Pr(X_1, ..., X_m)$ in terms of the relevant CPTs from the model. What is the size of the largest CPT in your expression?\n",
        "\n",
        "2.  Suppose that we want to predict the word $X_{m+1}$ given a sequence of $m$ words. We will use a model in which each word $X_i$ is conditioned on **all previous words** $X_1, ..., X_{i-1}$. Write an analytical expression for the joint probability $\\Pr(X_1, ..., X_m)$ in terms of the relevant CPTs from the model. What is the size of the largest CPT in your expression?\n",
        "\n",
        "3.  We again want to predict the word $X_{m+1}$ given a sequence of $m$ words. We will now use the **$n$-gram** model: $X_i$ is only conditioned on the  previous $n-1$ words. (A 1-gram model has no conditioning at all; each word is independent.) Write an analytical expression for the joint probability $\\Pr(X_1, ..., X_m)$ in terms of the relevant CPTs from the model. For ease of notation you may assume that $n \\geq 2$. What is the size of the largest CPT in your expression?\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um7KUiWqy4m9"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\n",
        "\n",
        "1.  $P(X_1, ... , X_m) = P(X_j) \\prod_{i=1}^{m} P(X_j | X_1, ..., X_{j-1}, X_{j+1}, ..., X_m)$. \\\\\n",
        "The size of the largest CPT is $|X|^2 = X^2$. \\\\\n",
        "\n",
        "2. $P(X_1, ..., X_m) = P(X_{m+1}) \\prod_{i=1}^{m} P(X_{m+1} | X_{1}, ..., X_m)$. \\\\\n",
        " The size of the largest CPT is $|X|^m$. \\\\\n",
        "\n",
        "\n",
        "3.  $P(X_1, ..., X_m) = P(X_{m+1}) \\prod_{i=1}^{m} P(X_{i} | X_{i-1}, ..., X_{i-1-n-1})$. \\\\\n",
        " The size of the largest CPT is $|X|^n$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQMAqpGGUu2g"
      },
      "source": [
        "# POS Tagging\n",
        "\n",
        "In this assignment you will explore [part-of-speech (POS) tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging), a standard task in natural language processing. The goal is to identify parts of speech and related labels for each word in a given corpus. HMMs are well suited for this problem, with parts of speech being hidden states and the words themselves being observations.\n",
        "\n",
        "We will be using data from the English EWT treebank from [Universal Dependencies](https://universaldependencies.org/treebanks/en_ewt/index.html), which uses 17 POS tags. We are providing clean versions of training and test data for you. The data format is such that each line contains a word and associated tag, and an empty line signifies the end of a sentence. Feel free to open the files in a text editor to get an idea.\n",
        "\n",
        "Start by uploading both files to the Jupyter session storage (you should do this each time that you start a new session). You can do so by clicking the Files tab on the left sidebar and then uploading from your computer. Then run the following code cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8YbOgtrrymZ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def read_sentence(f):\n",
        "  sentence = []\n",
        "  while True:\n",
        "    line = f.readline()\n",
        "    if not line or line == '\\n':\n",
        "      return sentence\n",
        "    line = line.strip()\n",
        "    word, tag = line.split(\"\\t\", 1)\n",
        "    sentence.append((word, tag))\n",
        "\n",
        "def read_corpus(file):\n",
        "  f = open(file, 'r', encoding='utf-8')\n",
        "  sentences = []\n",
        "  while True:\n",
        "    sentence = read_sentence(f)\n",
        "    if sentence == []:\n",
        "      return sentences\n",
        "    sentences.append(sentence)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m4b10sfi4WC"
      },
      "source": [
        "training = read_corpus('train.upos.tsv')\n",
        "TAGS = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', \n",
        "        'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
        "NUM_TAGS = len(TAGS)\n",
        "\n",
        "alpha = 0.1\n",
        "tag_counts = np.zeros(NUM_TAGS)\n",
        "transition_counts = np.zeros((NUM_TAGS,NUM_TAGS))\n",
        "obs_counts = {}\n",
        "\n",
        "for sent in training:\n",
        "  for i in range(len(sent)):\n",
        "    word = sent[i][0]\n",
        "    pos = TAGS.index(sent[i][1])\n",
        "    tag_counts[pos] += 1\n",
        "    if i < len(sent)-1:\n",
        "      transition_counts[TAGS.index(sent[i+1][1]), pos] += 1\n",
        "    if word not in obs_counts:\n",
        "      obs_counts[word] = np.zeros(NUM_TAGS)\n",
        "    (obs_counts[word])[pos] += 1\n",
        "\n",
        "X0 = tag_counts / np.sum(tag_counts)\n",
        "TPROBS = transition_counts / np.sum(transition_counts, axis=0)\n",
        "OPROBS = {'#UNSEEN': np.divide(alpha*np.ones(NUM_TAGS), tag_counts+alpha)}\n",
        "for word, counts in obs_counts.items():\n",
        "  OPROBS[word] = np.divide(counts, tag_counts+alpha)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W-eqo75bsxw"
      },
      "source": [
        "The preceding cell estimates the parameters of the HMM model by going through all the training data and counting each tag, word, and transition that appears. `X0` is a numpy array storing the initial distribution of tags. `TPROBS` is the transition matrix, in the form of a 2d numpy array. `OPROBS` is a dictionary of 1d numpy arrays, with keys as the words appearing in the training data and their values being 1d numpy arrays of the emission probabilities. \n",
        "\n",
        "Notice that we're including one extra \"tag\" in `OPROBS`: an `#UNSEEN` tag. This is necessary because we will inevitably encounter words in the test dataset that we have not seen before. For any word that we have not seen, we treat it as if the word is just `#UNSEEN`. We assign the \"count\" of an `#UNSEEN` word to a constant value $\\alpha$, so that it acts as Laplacian smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmfThHKofcPQ"
      },
      "source": [
        "## Coding 1 (5 points)\n",
        "\n",
        "Before we build our HMM, let's try a simplistic unigram model: given a single word, return the most likely POS tag. Context of surrounding words is therefore not considered. The probability of a tag given the word can be computed using Bayes' theorem:\n",
        "\n",
        "$\\Pr(\\text{tag} \\mid \\text{word}) \\propto \\Pr(\\text{word} \\mid \\text{tag}) \\Pr(\\text{tag})$\n",
        "\n",
        "The most likely tag can then be found by taking the argmax of the above likelihood over all tags. Remember that we treat all words that did not appear before as ```#UNSEEN```. Complete the function below to achieve this (make sure to actually return the POS tag itself, not the tag index)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhZsS0vfoC7t"
      },
      "source": [
        "def unigram(obs):\n",
        "  # Returns the tag of the word obs, as predicted by a unigram model\n",
        "  # YOUR CODE HERE\n",
        "  max = -float(\"inf\")\n",
        "  if OPROBS.get(obs) is None:\n",
        "    prob = np.multiply(X0, OPROBS.get(\"#UNSEEN\"))\n",
        "  else:\n",
        "    prob = np.multiply(X0, OPROBS.get(obs))\n",
        "  for i in range(0,len(prob)):\n",
        "    if prob[i] > max:\n",
        "      max = prob[i]\n",
        "      index = i\n",
        "  return TAGS[index]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5IuwxZmhGx_"
      },
      "source": [
        "Test out your unigram model by running the cell below, which will tag the specified data and compute accuracy rates over all words and unseen words only. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_28iq5OhHC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe13eb9-3ecf-40df-9c26-ab4b3bc5152b"
      },
      "source": [
        "def evaluate(sentences, method):\n",
        "  correct = 0\n",
        "  correct_unseen = 0\n",
        "  num_words = 0\n",
        "  num_unseen_words = 0\n",
        "\n",
        "  for sentence in sentences:\n",
        "    words = [sent[0] for sent in sentence]\n",
        "    pos = [sent[1] for sent in sentence]\n",
        "    unseen = [word not in OPROBS for word in words]\n",
        "    if method == 'unigram':\n",
        "      predict = [unigram(w) for w in words]\n",
        "    elif method == 'viterbi':\n",
        "      predict = viterbi(words)\n",
        "    else:\n",
        "      print(\"invalid method!\")\n",
        "      return\n",
        "\n",
        "    if len(predict) != len(pos):\n",
        "      print(\"incorrect number of predictions\")\n",
        "      return\n",
        "    correct += sum(1 for i,j in zip(pos, predict) if i==j)\n",
        "    correct_unseen += sum(1 for i,j,k in zip(pos, predict, unseen) if i==j and k)\n",
        "    num_words += len(words)\n",
        "    num_unseen_words += sum(unseen)\n",
        "  \n",
        "  print(\"Accuracy rate on all words: \", correct/num_words)\n",
        "  if num_unseen_words > 0:\n",
        "    print(\"Accuracy rate on unseen words: \", correct_unseen/num_unseen_words)\n",
        "\n",
        "print(\"Training data evaluation\")\n",
        "evaluate(training, 'unigram')\n",
        "test = read_corpus('test.upos.tsv')\n",
        "print(\"\")\n",
        "print(\"Test data evaluation\")\n",
        "evaluate(test, 'unigram')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data evaluation\n",
            "Accuracy rate on all words:  0.9229889495471807\n",
            "\n",
            "Test data evaluation\n",
            "Accuracy rate on all words:  0.8569151691437223\n",
            "Accuracy rate on unseen words:  0.3102094240837696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuG-HJE54TfN"
      },
      "source": [
        "## Response 1 (5 points)\n",
        "\n",
        "You should see that accuracy on the training data is about 92\\%. Accuracy on the test data set is lower at about 85.7\\%, with accuracy on unseen words only about 31\\%. What does this last number mean? Your answer should be something like \"31\\% of [GROUP] have a POS tag of [TAG]\". Briefly explain your reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62lnToxylBbn"
      },
      "source": [
        "ENTER YOUR RESPONSE HERE\n",
        "31% of words in the test set have a POS tag of NOUN. The test set is made of unseen words, thus the resulting tag is a consequence of the algorithm trying to approximate some tag even though the word hasn't been seen. Essneitally, when a word is unseen, the algorithm takes the argma of the initial probabilites of each tag; again, this is because there are no prior observations which we can pick a tag for this word fromm. The argmax of the TAGS distribution, X0, is 7, so the algorithm always tries the tag \"NOUN\", giving 31% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9mf3c7ymgiN"
      },
      "source": [
        "## Coding 2 (10 points)\n",
        "\n",
        "We will now implement Viterbi to improve our performance over the unigram model. We will split the implementation into several subroutines. First complete the ```elapse_time``` function below. Given a distribution ```m```, it should return an updated \"distribution\" that occurs when applying the Viterbi update in one timestep. In addition, it should also return a list with the *indices* of the most likely prior tag for each current tag.\n",
        "\n",
        "As a hint, these will correspond to ```max``` and ```argmax``` operations, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYnTF9ke87Ku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a01377-59a4-4214-82ee-6f76339e4f5c"
      },
      "source": [
        "def elapse_time(m):\n",
        "  \"\"\"\n",
        "  Given a \"message\" distribution over tags, return an updated distribution\n",
        "  after a single timestep using Viterbi update, along with a list of the \n",
        "  indices of the most likely prior tag for each current tag\n",
        "  \"\"\"\n",
        "  mprime = np.zeros(NUM_TAGS)\n",
        "  prior_tags = np.zeros(NUM_TAGS, dtype=np.int8)\n",
        "  #YOUR CODE HERE\n",
        "  size = len(m)\n",
        "  updated = np.zeros(shape=(size,size))\n",
        "  for i in range(0, len(updated)):\n",
        "    max = -float(\"inf\")\n",
        "    for j in range(0, len(updated)):\n",
        "      updated[i][j] = TPROBS[i][j] * m[j]\n",
        "      if max < updated[i][j]:\n",
        "        max = updated[i][j]\n",
        "        index = j\n",
        "    mprime[i] = max\n",
        "    prior_tags[i] = index\n",
        "\n",
        "  return mprime, prior_tags\n",
        "\n",
        "elapse_time(X0)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.01760448, 0.03555192, 0.00856414, 0.02886908, 0.01286977,\n",
              "        0.03107597, 0.00131929, 0.04729066, 0.00355399, 0.00759845,\n",
              "        0.01917727, 0.01772987, 0.05017312, 0.00486164, 0.00066568,\n",
              "        0.02484413, 0.00200137]),\n",
              " array([ 5,  7,  3, 10, 12,  1, 12,  5, 12, 15, 15, 11,  7, 15,  8, 10, 12],\n",
              "       dtype=int8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifBYfZpruH2l"
      },
      "source": [
        "You can check your implementation above by finding $m_1'$ given $m_0 = X_0$. It should approximately look like\n",
        "```\n",
        "[0.01760448, 0.03555192, 0.00856414, 0.02886908, 0.01286977,\n",
        " 0.03107597, 0.00131929, 0.04729066, 0.00355399, 0.00759845,\n",
        " 0.01917727, 0.01772987, 0.05017312, 0.00486164, 0.00066568,\n",
        " 0.02484413, 0.00200137]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPBgLK4Cpj5w"
      },
      "source": [
        "## Coding 3 (5 points)\n",
        "\n",
        "The second step of the Viterbi algorithm is to reweight probabilities according to observations. Given the \"time elapsed\" message distribution ```mprime``` and an ```obs``` (a word), the function below should return an updated distribution weighted by the emission probabilities for this observation. Remember to account for unseen words by using the ```#UNSEEN``` word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovQm2FmpBw0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574e29b9-7730-40bf-c643-726659705d70"
      },
      "source": [
        "def observe(mprime, obs):\n",
        "  \"\"\"\n",
        "  Given a \"message\" distribution over tags, return an updated distribution\n",
        "  by weighting mprime by the emission probabilities corresponding to obs\n",
        "  \"\"\"\n",
        "  m = np.zeros(NUM_TAGS)\n",
        "  # YOUR CODE HERE\n",
        "  if OPROBS.get(obs) is None:\n",
        "    prob = OPROBS.get(\"#UNSEEN\")\n",
        "  else:\n",
        "    prob = OPROBS.get(obs)\n",
        "  m = np.multiply(prob, mprime[0])\n",
        "\n",
        "  return m\n",
        "\n",
        "observe(elapse_time(X0), \"#UNSEEN\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.41071718e-07, 2.01551791e-07, 8.11836409e-08, 2.33699060e-07,\n",
              "       1.91882732e-07, 1.90801109e-07, 1.91728776e-07, 1.35974363e-07,\n",
              "       8.88920776e-08, 1.36488417e-07, 1.03230672e-07, 1.36951454e-07,\n",
              "       2.11887791e-07, 1.26568822e-07, 1.11112985e-07, 1.07680417e-07,\n",
              "       2.36260819e-07])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyn5QcGgvAAh"
      },
      "source": [
        "You can check your implementation above by finding $m_1$ given $m_1'$ (from your ```elapse_time``` test above) and an observation of your choosing. If the observation is an ```#UNSEEN``` word, you should find that $m_1$ looks something like \n",
        "```\n",
        "[1.41071718e-07 2.01551791e-07 8.11836409e-08 2.33699060e-07\n",
        " 1.91882732e-07 1.90801109e-07 1.91728776e-07 1.35974363e-07\n",
        " 8.88920776e-08 1.36488417e-07 1.03230672e-07 1.36951454e-07\n",
        " 2.11887791e-07 1.26568822e-07 1.11112985e-07 1.07680417e-07\n",
        " 2.36260819e-07]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLewyJPOrhPN"
      },
      "source": [
        "## Coding 4 (20 points)\n",
        "\n",
        "You will now put the two functions you wrote above together into one wrapper ```viterbi``` function. Given a list of word observations, it should return a corresponding list of predicted tags. As you hopefully recall, Viterbi runs in two phases. The \"forward\" phase starts with the ```X0``` distribution and goes through the observations one at a time, modifying the message distribution through time and observation updates for each. You should maintain a growing list of most likely tag pointers that is returned from each elapse time update.\n",
        "\n",
        "The second phase then goes backward. Starting with the tag with the highest likelihood at the end, follow the pointers backward to find the most likely tag for each observation. These tags should then be returned as one overall list of tags when finished (make sure the list of tags is in the right order!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xqPy45-E3hX"
      },
      "source": [
        "def viterbi(observations):\n",
        "  \"\"\"\n",
        "  Given a list of word observations, return a list of predicted tags\n",
        "  \"\"\"\n",
        "  m = X0\n",
        "  pointers = []\n",
        "  # YOUR CODE HERE\n",
        "  # \"Forward\" phase of the Viterbi algorithm\n",
        "  for obs in observations:\n",
        "    elapsed_result = elapse_time(m)\n",
        "    pointers.append(elapsed_result[1])\n",
        "    m = observe(elapsed_result, obs)\n",
        "  # \"Backward\" phase of the Viterbi algorithm\n",
        "  highest = np.argmax(m)\n",
        "  sequence = []\n",
        "  for pts in pointers[::-1]:\n",
        "    sequence.append(TAGS[highest])\n",
        "    highest = pts[highest]\n",
        "    \n",
        "  return sequence[::-1]\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkSnP9YAv5qE"
      },
      "source": [
        "Again, we recommend that you check your implementation with small test cases. For example, running ```viterbi(['#UNSEEN'])``` (or any unseen word in place of ```#UNSEEN```) should return ```['X']```, which means that we are unable to determine the POS tag for a previously unseen word without any context.\n",
        "\n",
        "## Response 2 (5 points)\n",
        "\n",
        "Run your Viterbi implementation on the three phrases below. Explain why the tag prediction is different for the word ```'round'``` in each of them. You can refer to the tag descriptions on UD's [website](https://universaldependencies.org/u/pos/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iOCsW2szn4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6055435d-94f6-49ec-9151-1e475ccc0d1d"
      },
      "source": [
        "print(viterbi(['a', 'round', 'circle']))\n",
        "print(viterbi(['play', 'another', 'round']))\n",
        "print(viterbi(['walk', 'round', 'the', 'fence']))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DET', 'ADJ', 'NOUN']\n",
            "['VERB', 'DET', 'NOUN']\n",
            "['VERB', 'ADP', 'DET', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrOdemDNCT7N"
      },
      "source": [
        "ENTER YOUR RESPONSE HERE\n",
        "The viterbi algorithm is a probabilistic model. Recall that a probabilistic model is based on the theory of probability (and randomness plays a role in predicting future events). The opposite of probabilistic is deterministic which indicates that something can be predicted exactly, without any randomness. Since the viterbi algorithm is probabilistic, there is variability in the resulting POS that is returned and this is shown in the cases above: round can be classified as an adjective, noun, or adposition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qsUtrNFwJai"
      },
      "source": [
        "Once you are confident of your implementation, you can run it on the full training and test data sets. You should find that both accuracy rates are better than those of the unigram model, at about 95\\% for the training set and (more importantly) 88\\% for the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R30oE7Q9HsiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92482d62-a5e4-4a1a-8697-02e01d96b973"
      },
      "source": [
        "print(\"Training data evaluation\")\n",
        "evaluate(training, 'viterbi')\n",
        "print(\"\")\n",
        "print(\"Test data evaluation\")\n",
        "evaluate(test, 'viterbi')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data evaluation\n",
            "Accuracy rate on all words:  0.9527973138748919\n",
            "\n",
            "Test data evaluation\n",
            "Accuracy rate on all words:  0.8794676654580229\n",
            "Accuracy rate on unseen words:  0.30584642233856896\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}